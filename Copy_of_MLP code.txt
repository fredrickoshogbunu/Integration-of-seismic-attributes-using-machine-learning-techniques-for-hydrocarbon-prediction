{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of seismic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpauH1tfq_wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install featuretools\n",
        "!pip install datacleaner\n",
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSkiyQbc5Kgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layer one \n",
        "# https://drive.google.com/open?id=14eRPAFikV1yPs76x3pK1qK56sZZcoKgi\n",
        "# layer two \n",
        "# https://drive.google.com/open?id=1xY7tVoxKdMc9hUOiK2PhOIdBx4HUmKfP\n",
        "# layer 3\n",
        "# https://drive.google.com/open?id=1InT5pg5B1jIo2UxdEvkDo5qJJ_D1GVjE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQjKowy9rQ0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# featuretools for automated feature engineering\n",
        "import featuretools as ft\n",
        "# Visualize training history\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy\n",
        "# load pima indians dataset\n",
        "# dataset = numpy.loadtxt(\"H1_MLP Data label.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "# X = dataset[:,0:7]\n",
        "# Y = dataset[:,7]\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# https://drive.google.com/uc?id=\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '14eRPAFikV1yPs76x3pK1qK56sZZcoKgi'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n",
        "dataset = pd.read_csv('H1_MLP Data label.csv')\n",
        "# dataset = dataset.dropna() \n",
        "print (dataset.head())\n",
        "#clean Dataset\n",
        "from datacleaner import autoclean\n",
        "import pandas as pd\n",
        "\n",
        "#clean Dataset\n",
        "dataset = pd.read_csv('H1_MLP Data label.csv', sep=',')\n",
        "dataset = autoclean(dataset)\n",
        "my_data.to_csv('my_clean_train_data_layer_1.csv', sep=',', index=False)\n",
        "dataset = pd.read_csv('my_clean_train_data_layer_1.csv')\n",
        "\n",
        "# loading the dataset\n",
        "Y = dataset[' Prediction / output  label'] # Target variable\n",
        "X = dataset[['Feature 1','Feature 2','Feature 3','Feature 4','Feature 5']]\n",
        "\n",
        "# Make an entityset and add the entity\n",
        "es = ft.EntitySet(id = 'dataset')\n",
        "es.entity_from_dataframe(entity_id = 'data', dataframe = dataset, \n",
        "                         make_index = True, index = 'index')\n",
        "\n",
        "# Perform deep feature synthesis without specifying primitives\n",
        "features, feature_names = ft.dfs(entityset=es, target_entity='data', \n",
        "                                 max_depth = 2)\n",
        "\n",
        "features.head()\n",
        "# feature_matrix.head()\n",
        "\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=5, activation='relu'))\n",
        "model.add(Dense(1, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# describe the model\n",
        "# model.\n",
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.44, epochs=300, batch_size=30, verbose=0)\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P06BZL3-5jB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}